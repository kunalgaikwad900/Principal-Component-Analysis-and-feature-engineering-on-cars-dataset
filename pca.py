# -*- coding: utf-8 -*-
"""PCA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O8yDkxzE2Pl1ev69bULl4rKB4AYBkOOS
"""

# Commented out IPython magic to ensure Python compatibility.
# Data and stats packages
import numpy as np
import pandas as pd

# Visualization packages
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

#scikit-learn to implement PCA and preprocessing
from sklearn import datasets
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


# NEW PACKAGES
from sklearn.decomposition import PCA
from time import time


# Other packages
pd.set_option('display.max_columns', 50)
import warnings
warnings.filterwarnings("ignore")

#Mounting Google Drive to Project

from google.colab import drive
drive.mount('/content/gdrive')


import os 
os.chdir('/content/gdrive/MyDrive/Project')

#Loading datset from file

cars_df = pd.read_csv('mtcars.csv')
cars_df = cars_df[cars_df.columns.difference(['am', 'vs'])]
cars_df.head()

#brief description of dataset
cars_df.describe()

#Visualizing dependency of features by plotting them in pairs
sns.pairplot(cars_df);

# separating the quantitative predictors from the model of the car (a string)
model = cars_df['model']
quant_df = cars_df[cars_df.columns.difference(['model'])]

# Standardization (Preprocessing) dataset
quant_scaled = StandardScaler().fit_transform(quant_df)
cars_df_scaled = pd.DataFrame(quant_scaled, columns=quant_df.columns)

# We can bring back the model variable, although we do not need it
cars_df_scaled['model'] = cars_df['model']
cars_df_scaled.head()

#Brief description of scaled dataset
cars_df_scaled.describe()

# drop again the model predictor from dataset
PCA_df = cars_df_scaled[cars_df_scaled.columns.difference(['model'])]

# fitting the PCA object onto our updated dataframe (excluding the model name column)
pca = PCA().fit(PCA_df)

# transforming the dataframe
transf_df_pca = pca.transform(PCA_df)

print('Original dimensions:', quant_df.shape)
print('PCA dimensions:     ', quant_df_pca.shape)

#Calculating the variance explained by each feature for all features of dataset
fig, ax = plt.subplots(ncols=2, figsize=(20,6))
ax1, ax2 = ax.ravel()

ratio = pca.explained_variance_ratio_
ax1.bar(range(len(ratio)), ratio, color='purple', alpha=0.8)
ax1.set_title('Explained Variance Ratio PCA', fontsize=20)
ax1.set_xticks(range(len(ratio)))
ax1.set_xticklabels(['PC {}'.format(i+1) for i in range(len(ratio))])
ax1.set_ylabel('Explained Variance Ratio')

# ratio[0]=0
ratio = pca.explained_variance_ratio_
ax2.plot(np.cumsum(ratio), 'o-')

ax2.set_title('Cumulative Sum of Explained Variance Ratio PCA', fontsize=20)

ax2.set_ylim(0,1.1)
ax2.set_xticks(range(len(ratio)))
ax2.set_xticklabels(['PC {}'.format(i+1) for i in range(len(ratio))])
ax2.set_ylabel('Cumulative Sum of Explained Variance Ratio');

#We find that two features alone can explain 85% variation in data

#Displaying equations of principal components in PCA
for i, comp in enumerate(pca.components_):
    expression = 'Z_{} = '.format(i+1)
    for c, x in zip(comp, quant_df.columns):
        if c < 0:
            expression += str(np.round(c,2)) + '*' + x + ' '
        else:
            expression += '+' + str(np.round(c,2)) + '*' + x + ' '
    print(expression + '\n')

#Checking if vectors are orthogonal to each other
vec1 = pca.components_[0]
vec2 = pca.components_[1]

# print(np.dot(vec1.T, vec2))
print('The dot product between the first two principal components is ',np.round(np.dot(vec1, vec2),5))
print('The length of the first  principal component is ',np.round(np.dot(vec1, vec1),5))

#Visualizing dataset in 2D - 2 principal components
#to plot vectors from the center
vecs = pca.components_[0:10].T *2

fig, axis = plt.subplots(figsize=(16,8))
axis.plot(trans_df_pca[:,0], trans_df_pca[:,1], 'ok', markersize=4)
axis.set_xlabel('Principal Component 1')
axis.set_ylabel('Principal Component 2')
axis.set_title('Cars Dataset plotted using first 2 Principal Components', fontsize=20)

#plotting arrowheads of the original axes projected on the 2D PCA space
for i, vec in enumerate(vecs):
    axis.arrow(0,0,vec[0],vec[1], color='brown', head_width=0.1)
    s = 1.3
    axis.annotate(quant_df.columns[i], (s*vec[0], s*vec[1]), color='brown')

#annotating text
for i, text in enumerate(cars_df_scaled['model']):
    axis.annotate(txt, (trans_df_pca[:,0][i], trans_df_pca[:,1][i]), size=12)